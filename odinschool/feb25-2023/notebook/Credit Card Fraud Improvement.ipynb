{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported Libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82378e",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466188e",
   "metadata": {},
   "source": [
    "data is available under data/creditcard.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e17b8",
   "metadata": {},
   "source": [
    "## Improvement 1:  Balancing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ac7cf",
   "metadata": {},
   "source": [
    "Note: Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae0e68",
   "metadata": {},
   "source": [
    "### Random Under-Sampling:\n",
    "\n",
    "\n",
    "In this phase of the project we will implement \"Random Under Sampling\" which basically consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting.\n",
    "\n",
    "- Once we determine how many instances are considered fraud transactions (Fraud = \"1\") , we should bring the non-fraud transactions to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.\n",
    "- After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to shuffle the data to see if our models can maintain a certain accuracy everytime we run this script.\n",
    "\n",
    "Note: The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of information loss (bringing 492 non-fraud transaction from 284,315 non-fraud transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4337a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "#remaining data\n",
    "remaining_non_fraud_df = df.loc[df['Class'] == 0][493:]\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distribution of the Classes in the subsample dataset')\n",
    "print(new_df['Class'].value_counts()/len(new_df))\n",
    "\n",
    "\n",
    "sns.countplot(data=new_df, x=\"Class\")\n",
    "\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd00cc9",
   "metadata": {},
   "source": [
    "## Improvement 2:  Change Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_df.values\n",
    "X_test = X_test_df.values\n",
    "y_train = y_train_df.values\n",
    "y_test = y_test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc57803",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937de746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow our scores are getting even high scores even when applying cross validation.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    prec = precision_score(y_test, y_pred)  \n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    print(\"\\n\\nClassifiers: \", classifier.__class__.__name__)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f6077",
   "metadata": {},
   "source": [
    "## Right way to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_remaining = remaining_non_fraud_df.drop('Class', axis=1)\n",
    "Y_remaining = remaining_non_fraud_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e91a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_X = pd.concat([X_test_df, X_remaining])\n",
    "combined_test_Y = pd.concat([y_test_df, Y_remaining])\n",
    "combined_test_X = combined_test_X.values\n",
    "combined_test_Y = combined_test_Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(combined_test_X)\n",
    "    print(\"\\n\\nClassifiers: \", classifier.__class__.__name__)\n",
    "    print(classification_report(combined_test_Y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73823fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
